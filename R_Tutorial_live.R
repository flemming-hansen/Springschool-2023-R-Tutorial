# Basics of Data Analysis in R 
# Springschool Day 2, March 2023
# Merle Schuckart (merle.schuckart@gmx.de)

# ----------------------------------------------------------------

# Description of this script:

# This script will walk you through a very simple 
# data analysis (reading in data, preprocessing, stats & plotting).

# Description of the data used herein:
# The data we're analyzing here are from a simple experiment 
# in which I tried to replicate the redundant 
# signals effect (RSE). The RSE is a well-known effect, 
# according to which a participant should react faster to 
# multisensory stimuli than to unisensory ones.

# All files used herein are test data generated by me, 
# so no worries about privacy concerns here. 

# There are 3 datasets from minors (Ari, Uzi & Buckley Tenenbaum).
# The datasets of Richie, Uzi & Buckley Tenenbaum should contain a lot of missing data.

# All datasets contain information on gender, 
# age and id as well as the experimental data (RTs = "duration").
# The experiment was conducted offline using a lab.js study.

# Variables: 
# A  = 1 unisensory audio-stimulus
# V  = 1 unisensory visual stimulus
# VA = 1 multisensory audio-visual stimulus

# ----------------------------------------------------------------

# 1. Settings: 

# turn off scientific notation
options(scipen = 999)

# clear environment
remove(list = ls())

# create a list with needed libraries
pkgs <- c("yarrr",   # for plots
          "lattice", # for ugly but fast density plots
          "car",     # for Shapiro-Wilk-Test
          "ez")      # for ANOVA

# load each listed library, check if it's already installed 
# and install if necessary
for (pkg in pkgs){
  if(!require(pkg, character.only = TRUE)){
    install.packages(pkg)
    library(pkg, character.only = TRUE)
  }
}

rm(pkg, pkgs) # clean up helper variables


# ----------------------------------------------------------------

# 2. Load data and do some preprocessing

# 2.1 set working directory (= path to the file your data are in)
# set your own path here:
setwd('/Users/merle/Github/SpringSchool2023/Track1/day2/RSE_data') 
# Please notice we're always using normal slashes /, 
# backslashes \ don't work here.
# --> Have an eye on this if you're a Windows user!

# 2.2 Get the list of files in the directory
file_list <- list.files(pattern='.csv') 
# Hint: if you want to set a path to a folder here, use the argument path = " your_path "


# Loop through files to read in data

# 2.3
# Create empty dataframes as placeholder for 
# the df where you collect all our data & 
# one for the demographic information
df_data         <- data.frame() 
df_demographics <- data.frame()

# 2.4 loop datasets aka participants:
for (i in 1:length(file_list)) { # for the number of files in our directory...
  
  # 2.5 Get dataframe for one participant
  subj_df <- read.csv(file_list[i], sep = ",")

  # Find out in which row we have to look for the demographics:
  row_demogr <- which(subj_df$sender == "Hello") # find the row where sender is Hello
  
  # 2.6 Get participant id (in our case: Name)
  # --> Don't forget the comma in the bracket!
  id <- subj_df[row_demogr, ]$id # get value where column name is "code" and row name is Demographics
  
  # 2.7 Get age 
  age <- subj_df[row_demogr, ]$age # get value where column name is "age" and row name is Demographics
  
  # 2.8 Get gender (even shorter!)
  gender <- subj_df[row_demogr, ]$gender

  # 2.9 If person is a minor, the data should be 
  # excluded from further analysis.
  # For now, we just mark them so we can exclude them later
  if (age < 18) { 
    exclude_participant <- T
  } else {
    exclude_participant <- F
  }
  
  # We could also use multiple conditions to exclude people. 
  # For example, we could exclude all people who are female or aged 60 & over:
  #if (gender == "female" | age >= 60) { 
  #  exclude_participant <- T
  #} else {
  #  exclude_participant <- F
  #} 
 
  # But we don't do that here. :-)
  
  # ----------------------------------------------------------------
  
  # 2.10 Now get data (Reaction times (= RTs) and condition names):
  
  # get position of column with reaction times (durations) and condition names
  col_cond <- which(names(subj_df) == "condition")
  col_RTs <- which(names(subj_df) == "duration")
  
  # get only a part of subj_df
  RT_data <- subset(subj_df, sender == "Reaction")[c(col_cond, col_RTs)] # get only a part of subj_df
  
  # RT_data is now a matrix with 2 columns, but we have to covert it to a dataframe
  RT_data <- as.data.frame(RT_data)
  
  # uncomment this line and run it to have a look at your df:
  #View(RT_data)
  
  # You can see that the column with the reaction times is still called 
  # "duration" as in the original csv file, so we rename it:
  names(RT_data) <- c("condition", "reaction_time")
  
  # 2.11 Exclude all trials in which the reaction time is either 
  # too short (< 100 ms) or too long (> 500 ms, this is an arbitrary value btw).
  # If the participant didn't react, the RT is about 1500 ms, 
  # so we kick non-reactions out as well with these criteria.
  RT_data_clean <- subset(RT_data, reaction_time >= 100 & reaction_time <= 500)
  
  # how many trials did we exclude because they were too short or too long?
  trials_excluded <- length(RT_data$reaction_time) - length(RT_data_clean$reaction_time)
  # show this as a message when you run the script
  message( paste("excluded" , trials_excluded, "trials") ) # with paste() you can merge strings together!
  
  # If the participant always reacted too slow or didn't 
  # react in general (in this case the RT is saved as 1500 ms aka too long),
  # we need to exclude them, too.
  if (length(RT_data_clean$reaction_time) < 80){
    exclude_participant <- T
  }
  
  # ----------------------------------------------------------------
  
  # 2.12 Create 2 dfs: 
  # One with demographic information & number of excluded trials, 
  # one with the RT-data, conditions and IDs.
  
  # df 1:
  # combine all information we collected about the participant in a
  # df consisting of 1 row and 8 columns 
  dem <- as.data.frame(cbind(id, age, gender, 
                             exclude_participant, trials_excluded))
  
  # Remember the empty dfs we defined before the loop 
  # where we wanted to collect our demographical data & RTs?
  # Add our small df for the current subject as new row to the demographical 
  # data df from the beginning so there's 1 row for each participant:
  df_demographics <- as.data.frame(rbind(df_demographics, dem))
  
  # df 2: 
  # create a vector that contains the subject ID, but repeat it 
  # so there's 1 value for each row in RT_data_clean:
  participant_ID <- c(rep(id, times = length(RT_data_clean$condition)))
  # append the ID vector to RT_data_clean 
  RT_data_clean <- as.data.frame(cbind(participant_ID, RT_data_clean))
  # append RT_data_clean to big df containing data from all participants
  df_data <- as.data.frame(rbind(df_data, RT_data_clean))
  
} # END loop files in directory  

# 2.13 clean up a bit: 
# remove everything from environment except for the things we still need
#rm(list = setdiff(ls(), c("df_data", "df_demographics")))

# Uncomment this to have a look at your dataframes: 
#View(df_data)
#View(df_demographics)

# ----------------------------------------------------------------

# 3. Preprocessing

# 3.1 Exclude all participants who were marked before as to be excluded.

# Find out in which rows we set "exclude_participant" to TRUE:
pos_excl <- which(df_demographics$exclude_participant == T)
# get the IDs from those rows - those are the 
# participants we want to kick out (lovingly ofc):
id_excl <- df_demographics[pos_excl,]$id

# copy df_data and name the copy df_data_clean
df_data_clean <- df_data

# loop ids, only get subset of df_data_clean 
# where the id is not(!) the id we want to exclude
for (id in id_excl){
  df_data_clean <- subset(df_data_clean, participant_ID != id)
}

# If we now check the unique IDs left in df_data_clean, we don't see 
# "Richie Tenenbaum", "Ari Tenenbaum", "Uzi Tenenbaum" and
# "Buckley Tenenbaum" anymore. 
# Uncomment to have a look:
unique(df_data_clean$participant_ID)

# ----------------------------------------------------------------

# 3.2 Check distribution of our raw data

# get subset of data for each condition:
subset_v  = subset(df_data_clean, condition == "V")$reaction_time
subset_a  = subset(df_data_clean, condition == "A")$reaction_time
subset_va = subset(df_data_clean, condition == "VA")$reaction_time

# plot dstributions as density plots:
plot(density(subset_v),   # data for condition V
     ylim = c(0, 0.015),  # set y-axis limits for plot
     xlim = c(0, 400),    # set x-axis limits for plot
     col = "indianred4")  # set colour for condition V
lines(density(subset_a),  # data for condition A
      col = "indianred3") # set colour for condition A
lines(density(subset_va), # data for condition VA
      col = "seagreen")   # set colour for condition VA

# If you have skewed data, you have 3 options: 
#   (a) trying to transform them to get a more symmetrical shape
# [ (b) resampling your data to get a normal distribution (a bit too complicated for this R intro course) ]
#   (c) using non-parametrical tests later


# (a) Transforming your data:

# Sometimes our data don't have a perfectly symmetrical bell shape (and by sometimes I mean always). 
# This can be a bit tricky because a lot of statistical tests assume that our data have a normal distribution.

# If the distribution looks like it leans to the left (i.e. it has a tail on the right), 
# it's positively skewed. RT data look like this.
# If it leans to the right (i.e. it has a tail on the left), 
# we have negatively skewed data.

# For positively skewed data like our RTs, a lot of people use a sqrt- or a log-transformation.
# When log-transforming your data, you take the logarithm of each value.
# Log-transforming basically shrinks large values more than smaller values.
# compare these values for example:
#log(200)
#log(20)
#log(2)

# The sqrt-transform does the same, just not as extreme as the log-transformation.
#sqrt(200)
#sqrt(20)
#sqrt(2)

# In both cases this means that if we have a long right tail (aka a lot of smaller values and some large outliers), 
# the outliers get "pulled" to the left side a bit which makes the distribution look more bell-shaped.
# While this can be nice, this can also cause problems if you make your outliers match your distribution more.

# Just to show you how we would transform our data:
# log-transform data:
df_data_clean$log_RTs <- log(df_data_clean$reaction_time)

# get groups
subset_v_log  = subset(df_data_clean, condition == "V")$log_RTs
subset_a_log  = subset(df_data_clean, condition == "A")$log_RTs
subset_va_log = subset(df_data_clean, condition == "VA")$log_RTs

# plot them:
plot(density(subset_v_log),   # data for condition V
     col = "indianred4")      # set colour for condition V
lines(density(subset_a_log),  # data for condition A
      col = "indianred3")     # set colour for condition A
lines(density(subset_va_log), # data for condition VA
      col = "seagreen")       # set colour for condition VA


# There are other transformations that you can also use of course, 
# I prefer the z(sqrt(POMS(x))) transformation (see Berger & Kiefer, 2021).
# https://doi.org/10.3389/fpsyg.2021.675558
# You can also try rank-transforming your data using the rank() function
# if they are not normally distributed. 

# We will keep our log-transformed data for now because it's 
# a widely-used and easy-to-do transformation.

# ----------------------------------------------------------------

# 3.3 Excluding outliers

# When cleaning your data, you can remove outlier-trials for the whole sample, 
# for each group or each participant in each group.
# Afterwards, you can also aggregate your data (i.e. compute mean RTs for each participant 
# in each group for example) & exclude participants whose data deviate too much from the mean of your sample.
# What you do & which criteria you choose is completely up to you.

# 3.3.1 Removing outliers on participant x group level

# append column to df_data_clean that contains only FALSEs
# if a trial is an outlier, we will mark it as TRUE here
df_data_clean$excl_trial <- FALSE

# create vector containing participant IDs
ids <- unique(df_data_clean$participant_ID)

# create vector with our condition names
conds <- c("A", "V", "VA")

# loop participants in ids vector
for (id in ids){
  
  # loop conditions in condition names vector
  for (cond in conds){
    
     # get subset of data with the current condition
     curr_group <- subset(df_data_clean, condition == cond & id == id)

     # compute group mean & SD
     # --> Careful, M & SD only makes sense for symmetrical distributions. 
     # We pretend that ours are symmetrical enough (whatever that means).
     # plot(density(curr_group$log_RTs))
     
     # get mean & SD
     M  <- mean(curr_group$log_RTs)
     SD <- sd(curr_group$log_RTs)
     
     # mark all trials for exclusion that have a log RT < M - 2 * SD or > M + 2 * SD:
     outlier_idx <- which(df_data_clean$log_RTs < M - 2 * SD | df_data_clean$log_RTs > M + 2 * SD)
     df_data_clean[outlier_idx, "excl_trial"] <- TRUE
     
  }#END loop conditions
}#END loop participants

# now actually remove trials marked for exclusion
# only keep trials that are no outliers:
df_data_clean <- subset(df_data_clean, excl_trial == FALSE)


# 3.3.2 Exclude participants 
# We already excluded single trials, but what about participants who have completely 
# different RTs than the rest of the sample?
# Solution: exclude those, too.

# You can do this more or less exactly like we did with the single trials.
# I checked and there's no outlier participant, so I'll skip this step here (lucky you).

# steps you would have to take to do this:

# - Aggregate data and remove datasets of participants who deviate too much from the sample mean 
# - Find out if there are participants whose mean RTs are > M + 2*SD or < M - 2*SD
#   with M being the mean of all participants' means in one of the conditions and SD being the sample SD.
# - kick out participants who are outliers


# ----------------------------------------------------------------

# 4. Descriptive Stats

# aggregate the data again, but this time aggregate over participants:

agg_data_all <- aggregate(df_data_clean$log_RT, # which variable do you want to get means for?
                          by = list( df_data_clean$condition), # grouping factors: group only by condition
                          FUN = mean) # compute means

# assign nicer column names
names(agg_data_all) <- c("Condition", "Mean_log_RT")


# Have a look: Our hypotheses were that 
# 1. the RTs in the multisensory condition VA should be a little faster than in A and V. 
# 2. the RTs in V should be faster than in A. 
# Looks good!
# View(agg_data_all)

# You can also make some more plots here but let's skip that part.

# ----------------------------------------------------------------

# 5. Inferential Stats 

# 5.1 Shapiro-Wilk Test
# We have a super small sample, but we test the distribution here anyway so you know how to do it:

# Compute Shapiro-Wilk-test for normality of distribution. 
# If we get a significant result for one of the groups, 
# this means we don't have normality of distribution and we have to test 
# non-parametrically (either by rank-transforming our data or using non-parametrical tests).
# If we don't get a significant result, though, this means we couldn't show that our distribution 
# is NOT non-normally distributed. This doesn't mean it's normally distributed, but in practice we assume it does.

# (Careful here btw, normally you wouldn't test anything with such a small sample size.)

# If we don't get significant results, we can use 
# parametrical tests (e.g. ANOVAs and t-tests).
shapiro.test(subset(agg_data, Condition == "A")$Mean_log_RT)
# p = 0.6896, so not significant on 5% alpha level --> maybe normally distributed

shapiro.test(subset(agg_data, Condition == "VA")$Mean_log_RT)
# p = 0.4906, so not significant on 5% alpha level --> maybe normally distributed

shapiro.test(subset(agg_data, Condition == "V")$Mean_log_RT)
# p = 0.9087, so significant on 5% alpha level --> maybe normally distributed

# --> All tests are significant on a 5% alpha level, so we can use parametrical tests.


# 5.2 Levene Test
# Normality of distribution is probably given (at least that's what the Shapiro-Wilk tests hint to), 
# so check homogeneity of variance (--> Levene test) and sphericity (Mauchly's test).
# First the leve test: It checks whether your variances don't differ between your groups.
# If this test is significant, your variances differ between the groups. 
leveneTest(data = agg_data, Mean_log_RT ~ as.factor(Condition))
# p = 0.6901, aka not significant on a 5% alpha level --> use parametrical tests

# I know I said we'll compute the Mauchly test, but we'll do that together with the ANOVA:


# 5.3 ANOVA
# Is there a difference between the RTs in A, V and VA?
ANOVA_res <- ezANOVA(data   = agg_data,    # your dataframe
                     dv     = Mean_log_RT, # dv = dependent variable = AV
                     wid    = ID,          # case identifier = ID
                     within = Condition)   # independent variable = UV

# The ezANOVA function is super useful because it 
# computes the Mauchly test (--> test for sphericity) 
# for us as well. 

# To test whether you have sphericity in your data, you compute the differences between the 
# values in your groups and then you run something like the Levene test 
# where you check whether the variances of your differences are equal.

# If the Mauchly test is significant, 
# we need to use a Greenhouse-Geisser corrected p-value.
# Don't worry about this, the ezANOVA gives us 
# corrected p-value as well, so we only have to check if we need 
# to report the corrected or the uncorrected one. Easy peasy! ;-) 

# If the Mauchly test was not significant, get uncorrected values:
if (ANOVA_res$`Mauchly's Test for Sphericity`$p <= 0.05){
  p_val <- ANOVA_res$`Sphericity Corrections`$`p[GG]`
} else {
  p_val <- ANOVA_res$ANOVA$p
}

# Get the test statistic F and the degrees of freedom as well
F_val <- ANOVA_res$ANOVA$`F`
# save degrees of freedom values as 1 string
df <- paste(as.character(ANOVA_res$ANOVA$DFn), ", ", as.character(ANOVA_res$ANOVA$DFd), sep = "")

# round values a bit!
p_val <- round(p_val, digits = 3)
F_val <- round(F_val, digits = 3)

# Save the results in a new dataframe!
df_results <- as.data.frame(cbind("ANOVA", "A, V & VA", p_val, F_val, df))

# Our ANOVA was significant, so we can now use post-hoc t-tests to 
# find out which groups are significantly different.

# --------------

# Quick stats explanation concerning the values we compute here: 

# degrees of freedom tell you how many independent pieces of information you had to do your calculations.
#   In a test where you only compare 2 conditions (a t-test for example), your df = sample size - 1
#   In a test with more groups, like our ANOVA, you have 2 dfs, 
#   with df1 being number of groups - 1 and 
#   df2 being number of all observations across groups - number of groups.

# The test statistic (F in ANOVAs, t in t-tests, W in Wilcox tests,...) tells you 
#   how different your actual distribution is from a distribution where your groups don't differ. 
#   So it basically tells you how closely your data match a distribution you would expect if the H0 was true. 
#   The larger the value, the larger the difference between your data and the H0. 
#   That's why for a test statistic that exceeds a certain cutoff value, you assume your groups differ significantly.
#   To calculate the test statistic, you divide the similarity of your groups (e.g. difference between means) 
#   by how much your data vary (e.g. standard deviation).

# The p-value just tells you whether your groups are significantly different. 
#   p <= alpha means your groups are significantly different, with alpha being 
#   the level of significance (almost always 0.05 = 5%). If you compare multiple groups with each other, 
#   you have to correct the p-value up a bit, for example by multiplying 
#   it with the number of tests you ran on the same data (Bonferroni correction).

# We need all these values if we want to report our results somewhere, 
# that's why we collect them in a df here.


# ----------------------------------------------------------------


# 5.4 Post-hoc tests! 
# (use t-tests for dependent groups)

# 4.4.1 Difference between V and VA: Is RT in V > VA?
ttest_V_VA <- t.test(subset(agg_data, Condition == "V")$Mean_log_RT, # H1: is > than...
                     subset(agg_data, Condition == "VA")$Mean_log_RT, 
                     alternative = "greater",
                     paired = T, # dependent sample, so T
                     exact = F)
# get results:
p_val <- round(ttest_V_VA$p.value * 3, digits = 3) # Bonferroni-Holm correction for multiple comparisons
# p-value = 0.005 aka significant difference here!
F_val <- round(ttest_V_VA$statistic, digits = 3)
df <- ttest_V_VA$parameter 

# add to results df! 
df_results <- as.data.frame(rbind(df_results, cbind("t-Test", "V > VA", p_val, F_val, df)))


# 4.4.2 Difference between A and VA: Is RT in A > VA?
ttest_A_VA <- t.test(subset(agg_data, Condition == "A")$Mean_log_RT, # H1: is > than...
                     subset(agg_data, Condition == "VA")$Mean_log_RT, 
                     alternative = "greater",
                     paired = T, # dependent sample, so T
                     exact = F)
# get results:
p_val <- round(ttest_A_VA$p.value * 3, digits = 3) # Bonferroni-Holm correction for multiple comparisons
# p-value = 0.000002408008 aka significant difference here!
F_val <- round(ttest_A_VA$statistic, digits = 3)
df <- ttest_A_VA$parameter 

# add to results df! 
df_results <- as.data.frame(rbind(df_results, cbind("t-Test", "A > VA", p_val, F_val, df)))


# 4.4.3 Difference between A and V: Is RT in A > V?
ttest_A_V <- t.test(subset(agg_data, Condition == "A")$Mean_log_RT, # H1: is > than...
                    subset(agg_data, Condition == "V")$Mean_log_RT, 
                    alternative = "greater",
                    paired = T, # dependent sample, so T
                    exact = F)
# get results:
p_val <- round(ttest_A_V$p.value * 3, digits = 3) # Bonferroni-Holm correction for multiple comparisons
# p-value = 0.02275132 aka significant difference here!
F_val <- round(ttest_A_V$statistic, digits = 3)
df <- ttest_A_V$parameter 

# add to results df! 
df_results <- as.data.frame(rbind(df_results, cbind("t-Test", "A > V", p_val, F_val, df)))


# ----------------------------------------------------------------

# 6. save preprocessed data & results df as csv:

write.csv(df_results, file = "/Users/merle/Github/SpringSchool2023/Track1/day2/df_results.csv")
write.csv(agg_data, file = "/Users/merle/Github/SpringSchool2023/Track1/day2/preprocessed_aggregated_data.csv")

# ----------------------------------------------------------------

# 7. Plots
# Now we want to plot our results so the world can see all this glory.

# You can use this website to get pretty colors for your plot:
# https://www.color-hex.com/

pirateplot(formula = Mean_log_RT ~ Condition, # plot RTs by condition
           data = agg_data, # your dataframe
           theme = 1, # choose 1 of 4 themes
           
           # customize plot!
           back.col     = "white", # set colour of background
           gl.col       = "white", # set colour of gridlines
           # pal        = "pony", # you can either use a colour palette
           pal          = c("powderblue", "chocolate3", "orangered3"), # or your own colours for the beans
           avg.line.col = "gray22", # average line colour
           bean.f.o     = 1,   # how transparent should the beans be?
           point.o      = 1,   # how transparent should the points be?
           point.pch    = 21,  # style of points
           point.cex    = 0.8, # size of points
           inf.method   = "ci", # plot confidence interval as box around M
           ylab         = "mean log(RT)", # label for y-axis
           xlab         = "Condition",    # label for x-axis
           ylim         = c(5.22, 5.44), # set limits of y-axis
           main         = "THIS IS F***ING SIGNIFICANT!") # humble title

# There are also a lot of cool examples for plots 
# on the internet, most of them with code you can copy 
# --> check out pirateplots from the yarrr package (we used this for the plot above) 
# or - if you're a little more advanced - the R Graph Gallery: 
# https://www.r-graph-gallery.com/ggplot2-package.html
# The easystats package also has really nice plots if you're working with LMMs. :-)


